#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# overview of txt.file
import pandas as pd
f=open ("dialogues2.txt")
raw=f.read()
type(raw)
len(raw)


# In[ ]:


# Tokenization, Lemmatization, and POS tagging
import nltk 
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize, sent_tokenize 
stop_words = set(stopwords.words('english')) 
  
tokenized = sent_tokenize(raw)
for i in tokenized: 
    wordsList = nltk.word_tokenize(raw) 
  
    wordsList = [w for w in wordsList if not w in stop_words]  
   
    tagged = nltk.pos_tag(wordsList) 
  
    print(tagged) 


# In[2]:


# summarization
import gensim
from smart_open import smart_open
from gensim.summarization import summarize, keywords
from pprint import pprint

text = " ".join((line for line in smart_open('dialogues2.txt', encoding='utf-8')))

pprint(summarize(text, word_count=20))

print(keywords(text))


# In[17]:


# create bag of words, dictionary, and corpus
import gensim
from gensim import corpora
from gensim.utils import simple_preprocess
from smart_open import smart_open
import nltk
from nltk.corpus import stopwords
stop_words = stopwords.words('english')

class BoWCorpus(object):
    def __init__(self, path, dictionary):
        self.filepath = path
        self.dictionary = dictionary

    def __iter__(self):
        global mydict  
        for line in smart_open(self.filepath, encoding='latin'):
            # tokenize
            tokenized_list = simple_preprocess(line, deacc=True)

            # create bag of words
            bow = self.dictionary.doc2bow(tokenized_list, allow_update=True)

            # update the source dictionary (OPTIONAL)
            mydict.merge_with(self.dictionary)

            # lazy return the BoW
            yield bow


bow_corpus = BoWCorpus('dialogues2.txt', dictionary=mydict)  # memory friendly

for line in bow_corpus:
    print(line)


# In[14]:


# Topic analysis
from gensim.models import LsiModel
from pprint import pprint

lsi_model = LsiModel(corpus=corpus, id2word=loaded_dict, num_topics=7, decay=0.5)

pprint(lsi_model.print_topics(-1))


# In[16]:


# Sentiment analysis
import nltk

def format_sentence(sent):
    return({word: True for word in nltk.word_tokenize(sent)})

pos = []
with open("./dialogues2.txt") as f:
    for i in f: 
        pos.append([format_sentence(i), 'pos'])

neg = []
with open("./dialogues2.txt") as f:
    for i in f: 
        neg.append([format_sentence(i), 'neg'])

training = pos[:int((.8)*len(pos))] + neg[:int((.8)*len(neg))]
test = pos[int((.8)*len(pos)):] + neg[int((.8)*len(neg)):]

from nltk.classify import NaiveBayesClassifier

classifier = NaiveBayesClassifier.train(training)

classifier.show_most_informative_features()


# In[ ]:


# Korean NLP challenge :

# 1.The agglutinative morphology with many suffixes and prefixes changes mood and tense in Korean.
# 2.The honorific words are extensively used to express a speaker or writer's relationship.
# 3.Regardless of spacing and the order of word, the meaning is almost same, which makes it hard to predict. 
# Thus, the processing such as lemmatization and tagging, etc. that I’ve done here is tricky for training and modeling Korean language. 

# For example, ‘Let's go on a trip during the holidays.’

    #‘우리연휴에는여행을가봐요.’
    #‘우리 연휴에는 여행을 가봐요.’ 
    #‘연휴에 우리 여행 가봐요.’   
    #‘우리 여행을 가봐요 연휴에는 .’
    #‘여행을 가봐요 연휴에는 우리.’
    
#from konlpy.tag import Okt  
#okt=Okt()  
#print(okt.morphs("우리 연휴에는 여행을 가봐요"))
#[‘'우리', ',', '연휴', '에는', '여행', '을', '가봐요']  
#print(okt.pos("우리 연휴에는 여행을 가봐요"))  
#('우리', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa’), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]  
#print(okt.nouns("우리 연휴에는 여행을 가봐요"))  
#[‘우리', '연휴', '여행']  

